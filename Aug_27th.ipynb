{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August 27th Meeting\n",
    "\n",
    "\n",
    "## Status\n",
    "\n",
    "1. I finished developping GPU kernels with shared memory for all 2nd order SBP operators\n",
    "    - Unify GPU kernels with tester function to test them all\n",
    "    - Verified all implemented functions\n",
    "    - FACEtoVOL and VOLtoFACE couldn't be implemented in GPU with good performance, so I used CPU version\n",
    "    \n",
    "    \n",
    "Sample Code:\n",
    "Sample 1.\n",
    "\n",
    "Unified GPU function names and variable names\n",
    " \n",
    "``` \n",
    "function D2x_GPU(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "end\n",
    "\n",
    "function D2x_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Dx_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Hxinv_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Hx_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function D2y_GPU(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "end\n",
    "\n",
    "function D2y_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Dy_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Hyinv_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "function Hy_GPU_shared(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Sample 2. tester_function\n",
    "\n",
    "```\n",
    "function tester_function(f,Nx,TILE_DIM_1,TILE_DIM_2,TILE_DIM)\n",
    "    Ny = Nx\n",
    "\t@show f\n",
    "\t@eval gpu_function = $(Symbol(f,\"_GPU\"))\n",
    "\t@eval gpu_function_shared = $(Symbol(f,\"_GPU_shared\"))\n",
    "\t@show gpu_function\n",
    "    @show gpu_function_shared\n",
    "    h = 1/Nx\n",
    "\t# TILE_DIM_1 = 16\n",
    "\t# TILE_DIM_2 = 2\n",
    "\n",
    "\tu = randn(Nx*Ny)\n",
    "\td_u = CuArray(u)\n",
    "\td_y = similar(d_u)\n",
    "\td_y2 = similar(d_u)\n",
    "\n",
    "\tgriddim = (div(Nx,TILE_DIM_1) + 1, div(Ny,TILE_DIM_2) + 1)\n",
    "\tblockdim = (TILE_DIM_1,TILE_DIM_2)\n",
    "\n",
    "\t# TILE_DIM = 32\n",
    "\tTHREAD_NUM = TILE_DIM\n",
    "    BLOCK_NUM = div(Nx * Ny,TILE_DIM)+1 \n",
    "    \n",
    "\ty = f(u,Nx,Ny,h)\n",
    "\t@cuda threads=THREAD_NUM blocks=BLOCK_NUM gpu_function(d_u, d_y, Nx, Ny, h, Val(TILE_DIM))\n",
    "    @cuda threads=blockdim blocks=griddim gpu_function_shared(d_u, d_y2, Nx, Ny, h, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "\t@show y ≈ Array(d_y)\n",
    "\t@show y ≈ Array(d_y2)\n",
    "\t@show y - Array(d_y2)\n",
    "\t\n",
    "\trep_times = 10\n",
    "\n",
    "\tt_y = time_ns()\n",
    "\tfor i in 1:rep_times\n",
    "\t\ty = f(u,Nx,Ny,h)\n",
    "\tend\n",
    "\tt_y_end = time_ns()\n",
    "\tt1 = t_y_end - t_y\n",
    "\n",
    "\tmemsize = length(u) * sizeof(eltype(u))\n",
    "\t@show Float64(t1)\n",
    "\t@printf(\"CPU Through-put %20.2f\\n\", 2 * memsize * rep_times / t1)\n",
    "\n",
    "\n",
    "\tprintln()\n",
    "\n",
    "\tt_d_y = time_ns()\n",
    "\tfor i in 1:rep_times\n",
    "\t\t@cuda threads=THREAD_NUM blocks=BLOCK_NUM gpu_function(d_u, d_y, Nx, Ny, h, Val(TILE_DIM))\n",
    "\t\t# @cuda threads=THREAD_NUM blocks=BLOCK_NUM D2y_GPU_v2(d_u, d_y, Nx, Ny, h, Val(TILE_DIM))\n",
    "\tend\n",
    "\tsynchronize()\n",
    "\tt_d_y_end = time_ns()\n",
    "\tt2 = t_d_y_end - t_d_y\n",
    "\t@show Float64(t2)\n",
    "\t@show Float64(t1)/Float64(t2)\n",
    "\t@printf(\"GPU Through-put (naive) %20.2f\\n\", 2 * memsize * rep_times / t2)\n",
    "\n",
    "\tprintln()\n",
    "\n",
    "\tt_d_y2 = time_ns()\n",
    "\tfor i in 1:rep_times\n",
    "\t\t@cuda threads=blockdim blocks=griddim gpu_function_shared(d_u, d_y2, Nx, Ny, h, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "\tend\n",
    "\tsynchronize()\n",
    "\tt_d_y2_end = time_ns()\n",
    "\tt3 = t_d_y2_end - t_d_y2\n",
    "\n",
    "\t@show Float64(t3)\n",
    "\t@show Float64(t1)/Float64(t3)\n",
    "\t@printf(\"GPU Through-put (shared memory)%20.2f\\n\", 2 * memsize * rep_times / t3)\n",
    "\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "2. I developed myMAT_beta_GPU!(u) function to assemble GPU kernels and work with conjugate_beta_GPU!()\n",
    "    - The output should be the left-hand-side A*u\n",
    "    - All GPU kernels can work and myMAT_beta_GPU!(u) can work with conjugate_beta_GPU!() to do conjugate gradient\n",
    "    - There are some bugs returning GPU arrays as outputs of the function myMAT_beta_GPU!(). Still trying to debug this part\n",
    "    - There are lots of CPU-GPU array conversions in the current version. Future work would require redesign of the structure and containers to reuse allocated resources better and reduce CPU-GPU array conversions\n",
    "    - Re-examined the previous matrix-free CPU code. There is room for optimization\n",
    "    \n",
    "    \n",
    "Sample code 3. myMAT_beta_GPU!()\n",
    "    \n",
    "```\n",
    "function myMAT_beta_GPU!(du::AbstractVector, u::AbstractVector, container, var_test) # , intermediates_GPU_mutable)\n",
    "    @unpack N, y_D2x, y_D2y, y_Dx, y_Dy, y_Hxinv, y_Hyinv, yv2f1, yv2f2, yv2f3, yv2f4, yv2fs, yf2v1, yf2v2, yf2v3, yf2v4, yf2vs, y_Bx, y_By, y_BxSx, y_BySy, y_BxSx_tran, y_BySy_tran, y_Hx, y_Hy = container\n",
    "    @unpack Nx,Ny,N,hx,hy,alpha1,alpha2,alpha3,alpha4,beta = var\n",
    "\n",
    "    N = Nx*Ny\n",
    "    cu_zeros = CuArray(zeros(N))\n",
    "    iGm = intermediates_GPU_mutable(Nx,Ny,N,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros,cu_zeros);\n",
    "\n",
    "    TILE_DIM_1 = 4\n",
    "    TILE_DIM_2 = 16\n",
    "\n",
    "    griddim_x = (div(Nx,TILE_DIM_1) + 1, div(Ny,TILE_DIM_2) + 1)\n",
    "    griddim_y = (div(Nx,TILE_DIM_2) + 1, div(Ny,TILE_DIM_1) + 1)\n",
    "\n",
    "    blockdim_x = (TILE_DIM_1,TILE_DIM_2)\n",
    "    blockdim_y = (TILE_DIM_2,TILE_DIM_1)\n",
    "\n",
    "    # @show typeof(u)\n",
    "    # @show typeof(iGm.du_x)\n",
    "    # @show blockdim_x\n",
    "    # @show griddim_x\n",
    "    # @show size(u)\n",
    "    # @show size(iGm.du_x)\n",
    "    @cuda threads=blockdim_x blocks=griddim_x D2x_GPU_shared(u,iGm.du_x, Nx, Ny, hx, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    # @show Array(iGm.du_x)\n",
    "    output = Array(iGm.du_x)\n",
    "    synchronize()\n",
    "    @cuda threads=blockdim_y blocks=griddim_y D2y_GPU_shared(u,iGm.du_y, Nx, Ny, hy, Val(TILE_DIM_2), Val(TILE_DIM_1))\n",
    "    synchronize()\n",
    "    du_ops = iGm.du_x + iGm.du_y\n",
    "    output2 = Array(du_ops)\n",
    "    @cuda threads=blockdim_y blocks=griddim_y BySy_GPU_shared(u,iGm.du1, Nx, Ny, hy, Val(TILE_DIM_2), Val(TILE_DIM_1))\n",
    "    synchronize()\n",
    "    iGm.du2 .= CuArray(VOLtoFACE_beta(Array(iGm.du1),1,Nx,Ny,N,yv2fs))\n",
    "    @cuda threads=blockdim_y blocks=griddim_y Hyinv_GPU_shared(iGm.du2,iGm.du3,Nx,Ny,hy, Val(TILE_DIM_2), Val(TILE_DIM_1))\n",
    "    synchronize()\n",
    "    iGm.du3 = alpha1 * iGm.du3\n",
    "\n",
    "    iGm.du5 = VOLtoFACE_beta(Array(iGm.du1),2,Nx,Ny,N,yv2fs)\n",
    "    @cuda threads=blockdim_y blocks=griddim_y Hyinv_GPU_shared(iGm.du5,iGm.du6,Nx,Ny,hy, Val(TILE_DIM_2), Val(TILE_DIM_1))\n",
    "    synchronize()\n",
    "    iGm.du6 = alpha2 * iGm.du6\n",
    "\n",
    "    iGm.du7 = CuArray(VOLtoFACE_beta(Array(u),3,Nx,Ny,N,yv2fs))\n",
    "    @cuda threads=blockdim_x blocks=griddim_x BxSx_tran_GPU_shared(iGm.du7,iGm.du8,Nx,Ny,hx,Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    @cuda threads=blockdim_x blocks=griddim_x Hxinv_GPU_shared(iGm.du8,iGm.du9,Nx,Ny,hx, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    iGm.du9 = beta * iGm.du9\n",
    "\n",
    "    @cuda threads=blockdim_x blocks=griddim_x Hxinv_GPU_shared(iGm.du7,iGm.du11,Nx,Ny,hx, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    iGm.du11 =alpha3 * iGm.du11\n",
    "\n",
    "    du12 = CuArray(VOLtoFACE_beta(Array(u),4,Nx,Ny,N,yv2fs))\n",
    "    @cuda threads=blockdim_x blocks=griddim_x BxSx_tran_GPU_shared(iGm.du12,iGm.du13,Nx,Ny,hx,Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    @cuda threads=blockdim_x blocks=griddim_x Hxinv_GPU_shared(iGm.du13,iGm.du14,Nx,Ny,hx,Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    iGm.du14 = alpha4 * iGm.du14\n",
    "    @cuda threads=blockdim_x blocks=griddim_x Hxinv_GPU_shared(iGm.du12,iGm.du16,Nx,Ny,hx,Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    iGm.du16 = alpha4 * iGm.du16\n",
    "    iGm.du0 = du_ops + iGm.du3 + iGm.du6 + iGm.du9 + iGm.du11 + iGm.du14 + iGm.du16\n",
    "    @cuda threads=blockdim_y blocks=griddim_x Hy_GPU_shared(iGm.du0,iGm.du17,Nx,Ny,hx,Val(TILE_DIM_1),Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    @cuda threads=blockdim_x blocks=griddim_x Hx_GPU_shared(iGm.du17,iGm.du,Nx,Ny,hx,Val(TILE_DIM_2),Val(TILE_DIM_2))\n",
    "    synchronize()\n",
    "    # return Array(iGm.du_x)\n",
    "    # @show output\n",
    "    output_final = Array(iGm.du_y)\n",
    "    return output_final\n",
    "    # return output2\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "Sample code 4. conjugate_beta_GPU() function\n",
    "\n",
    "```\n",
    "function conjugate_beta_GPU(myMAT_beta_GPU!,r,b,container,var,intermediate,maxIteration)\n",
    "    @unpack N, y_D2x, y_D2y, y_Dx, y_Dy, y_Hxinv, y_Hyinv, yv2f1, yv2f2, yv2f3, yv2f4, yv2fs, yf2v1, yf2v2, yf2v3, yf2v4, yf2vs, y_Bx, y_By, y_BxSx, y_BySy, y_BxSx_tran, y_BySy_tran, y_Hx, y_Hy = container\n",
    "    @unpack Nx,Ny,N,hx,hy,alpha1,alpha2,alpha3,alpha4,beta = var\n",
    "    # @unpack du_ops,du1,du2,du3,du4,du5,du6,du7,du8,du9,du10,du11,du12,du13,du14,du15,du16,du17,du0 = intermediate\n",
    "\n",
    "    # u = zeros(N);\n",
    "    # du = zeros(N);\n",
    "    u = CuArray(zeros(N))\n",
    "    du = CuArray(zeros(N))\n",
    "    tol = 1e-16\n",
    "\n",
    "    r .= b .- Array(myMAT_beta_GPU!(du,u,container,var))\n",
    "    p = copy(r)\n",
    "    Ap = similar(u)\n",
    "    rsold = r'*r\n",
    "    counts = 0\n",
    "    # maxIteration = 1000\n",
    "    for i = 1:maxIteration\n",
    "        Ap = Array(myMAT_beta_GPU!(du,CuArray(p),container,var))   # can't simply translate MATLAB code, p = r create a link from p to r, once p modified, r will be modified\n",
    "        Ap = Array(Ap)\n",
    "        alpha = rsold / (p'*Ap)\n",
    "        #u = u + alpha * p\n",
    "        axpy!(alpha,p,Array(u)) # BLAS function\n",
    "        #r = r - alpha * Ap\n",
    "        axpy!(-alpha,Ap,r)\n",
    "        rsnew = r'*r\n",
    "        if sqrt(rsnew) < tol\n",
    "            break\n",
    "        end\n",
    "        #p = r + (rsnew/rsold) * p\n",
    "        #p .= r .+ (rsnew/rsold) .*p\n",
    "        p .= (rsnew/rsold) .* p .+ r\n",
    "\n",
    "        rsold = rsnew;\n",
    "        counts += 1\n",
    "        #return rsold;\n",
    "    end\n",
    "    return u, counts\n",
    "end\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
