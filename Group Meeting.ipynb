{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Meeting September 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBP Finite Difference with GPU\n",
    "\n",
    "\n",
    "### SBP operators\n",
    "Summation-by-parts that mimic integration by parts property in a discrete sense.\n",
    "\n",
    "\n",
    "Sample:\n",
    "A 1D summation by parts operator 9 grid points\n",
    "```jl\n",
    "julia> Matrix(D2x)\n",
    "9×9 Array{Float64,2}:\n",
    " 64.0  -128.0    64.0     0.0     0.0     0.0     0.0     0.0   0.0\n",
    " 64.0  -128.0    64.0     0.0     0.0     0.0     0.0     0.0   0.0\n",
    "  0.0    64.0  -128.0    64.0     0.0     0.0     0.0     0.0   0.0\n",
    "  0.0     0.0    64.0  -128.0    64.0     0.0     0.0     0.0   0.0\n",
    "  0.0     0.0     0.0    64.0  -128.0    64.0     0.0     0.0   0.0\n",
    "  0.0     0.0     0.0     0.0    64.0  -128.0    64.0     0.0   0.0\n",
    "  0.0     0.0     0.0     0.0     0.0    64.0  -128.0    64.0   0.0\n",
    "  0.0     0.0     0.0     0.0     0.0     0.0    64.0  -128.0  64.0\n",
    "  0.0     0.0     0.0     0.0     0.0     0.0    64.0  -128.0  64.0\n",
    "  ```\n",
    "\n",
    "A 2D summation by parts operator 9 * 9 grid points\n",
    "\n",
    "```jl\n",
    "julia> Matrix(D2_x)\n",
    "81×81 Array{Float64,2}:\n",
    " 64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -128.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0  -128.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    " 64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  -128.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0  -128.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    64.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0    64.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  ⋮                             ⋮                                 ⋮    ⋱                             ⋮                             ⋮\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0       64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0     -128.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …     0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0   0.0\n",
    "  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0     0.0     0.0  …  -128.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  64.0\n",
    "  ```\n",
    "\n",
    "\n",
    "### Sparse Matrix Representation\n",
    "\n",
    "```jl\n",
    "julia> D2_x\n",
    "81×81 SparseMatrixCSC{Float64,Int64} with 243 stored entries:\n",
    "  [1 ,  1]  =  64.0\n",
    "  [10,  1]  =  64.0\n",
    "  [2 ,  2]  =  64.0\n",
    "  [11,  2]  =  64.0\n",
    "  [3 ,  3]  =  64.0\n",
    "  [12,  3]  =  64.0\n",
    "  [4 ,  4]  =  64.0\n",
    "  [13,  4]  =  64.0\n",
    "  [5 ,  5]  =  64.0\n",
    "  [14,  5]  =  64.0\n",
    "  [6 ,  6]  =  64.0\n",
    "  [15,  6]  =  64.0\n",
    "  [7 ,  7]  =  64.0\n",
    "  [16,  7]  =  64.0\n",
    "  [8 ,  8]  =  64.0\n",
    "  [17,  8]  =  64.0\n",
    "  [9 ,  9]  =  64.0\n",
    "  [18,  9]  =  64.0\n",
    "  [1 , 10]  =  -128.0\n",
    "  [10, 10]  =  -128.0\n",
    "  [19, 10]  =  64.0\n",
    "  [2 , 11]  =  -128.0\n",
    "  [11, 11]  =  -128.0\n",
    "  [20, 11]  =  64.0\n",
    "  ⋮\n",
    "  [79, 70]  =  -128.0\n",
    "  [62, 71]  =  64.0\n",
    "  [71, 71]  =  -128.0\n",
    "  [80, 71]  =  -128.0\n",
    "  [63, 72]  =  64.0\n",
    "  [72, 72]  =  -128.0\n",
    "  [81, 72]  =  -128.0\n",
    "  [64, 73]  =  64.0\n",
    "  [73, 73]  =  64.0\n",
    "  [65, 74]  =  64.0\n",
    "  [74, 74]  =  64.0\n",
    "  [66, 75]  =  64.0\n",
    "  [75, 75]  =  64.0\n",
    "  [67, 76]  =  64.0\n",
    "  [76, 76]  =  64.0\n",
    "  [68, 77]  =  64.0\n",
    "  [77, 77]  =  64.0\n",
    "  [69, 78]  =  64.0\n",
    "  [78, 78]  =  64.0\n",
    "  [70, 79]  =  64.0\n",
    "  [79, 79]  =  64.0\n",
    "  [71, 80]  =  64.0\n",
    "  [80, 80]  =  64.0\n",
    "  [72, 81]  =  64.0\n",
    "  [81, 81]  =  64.0\n",
    "  ```\n",
    "\n",
    "  \n",
    "## Matrix-free approach\n",
    "Instead of storing the matrix itself, we only store the effects of the matrix. Namely, we want to write a `D2_x()` function that achieves `D2_x(u) = D2_x * u`\n",
    "Here, u is an input vector.\n",
    "\n",
    "```jl\n",
    "function D2x(y_in, Nx, Ny, h)\n",
    "\tN = Nx*Ny\n",
    "\ty_out = zeros(N)\n",
    "\tidx = 1:Ny\n",
    "\ty_out[idx] = (y_in[idx] - 2 .* y_in[Ny.+ idx] + y_in[2*Ny .+ idx]) ./ h^2\n",
    "\n",
    "\tidx1 = Ny+1:N-Ny\n",
    "\ty_out[idx1] = (y_in[idx1 .- Ny] - 2 .* y_in[idx1] + y_in[idx1 .+ Ny]) ./ h^2\n",
    "\n",
    "\tidx2 = N-Ny+1:N\n",
    "\ty_out[idx2] = (y_in[idx2 .- 2*Ny] -2 .* y_in[idx2 .- Ny] + y_in[idx2]) ./ h^2\n",
    "\n",
    "\treturn y_out\n",
    "end\n",
    "```\n",
    "\n",
    "As you can verify, when we set Nx=Ny=9, h=1/8, this is achieves the result for y_in that `D2x(y_in, Nx, Ny, h) = D2_x * y_in`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Get rid of intermediate allocations \n",
    "We see y_out is allocated every time, we would like to get rid of this to reduce memory allocation by writting impure functions.\n",
    "\n",
    "```jl\n",
    "function D2x_beta!(y_in::Array{Float64,1}, Nx::Int64, Ny::Int64, N::Int64, hx::Float64, hy::Float64, y_out::Array{Float64,1})\n",
    "\t@inbounds  for idx = 1:Ny\n",
    "\t\ty_out[idx] = (y_in[idx] - 2*y_in[Ny + idx] + y_in[2*Ny + idx]) / hx^2\n",
    "\tend\n",
    "\n",
    "\t@inbounds for idx1 = Ny+1:N-Ny\n",
    "\t\ty_out[idx1] = (y_in[idx1 - Ny] - 2 * y_in[idx1] + y_in[idx1 + Ny]) / hx^2\n",
    "\tend\n",
    "\n",
    "\t@inbounds for idx2 = N-Ny+1:N\n",
    "\t\ty_out[idx2] = (y_in[idx2 - 2*Ny] -2 * y_in[idx2 - Ny] + y_in[idx2]) / hx^2\n",
    "\tend\n",
    "end\n",
    "```\n",
    "This function would ask you to provide an pre-allocated `y_out` vector and directly write results into this `y_out` for faster memory allocation.\n",
    "\n",
    "This is very close to GPU kernels where you can't not return a function output but you need to `return nothing` after kernel calls\t\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Writing naive GPU kernels\n",
    "\n",
    "\n",
    "```jl\n",
    "function D2x_GPU(y_in, y_out, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\tif tidx <= Ny\n",
    "\t\ty_out[tidx] = (y_in[tidx] - 2 * y_in[Ny + tidx] + y_in[2*Ny + tidx]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\tif Ny+1 <= tidx <= N-Ny\n",
    "\t\ty_out[tidx] = (y_in[tidx - Ny] - 2 .* y_in[tidx] + y_in[tidx + Ny]) / h^2\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\tif N-Ny+1 <= tidx <= N\n",
    "\t\ty_out[tidx] = (y_in[tidx - 2*Ny] -2 * y_in[tidx - Ny] + y_in[tidx]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\tnothing\n",
    "end\n",
    "```\n",
    "\n",
    "Here y_in and y_out needs to be GPU arrays CuArray. After running this kernel, the y_out would be modified directly.\n",
    "\n",
    "Accessing global CPU memory from GPU is expensive. It's very often the bottleneck of GPU computation. Because GPU is designed to maximize through-put but not minimize latency.\n",
    "We can reduce this latency by writing data to shared memory on GPU in bulk and carry out calculation using loaded memory.\n",
    "\n",
    "\n",
    "## Writing GPU kernels using Shared memory\n",
    "```jl\n",
    "function D2x_GPU_shared(y_in, y_out, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "    tidx = threadIdx().x\n",
    "    tidy = threadIdx().y\n",
    "\n",
    "    # for global memory indexing\n",
    "    i = (blockIdx().x - 1) * TILE_DIM1 + tidx\n",
    "    j = (blockIdx().y - 1) * TILE_DIM2 + tidy\n",
    "\n",
    "    global_index = i + (j - 1) * Ny\n",
    "\n",
    "    HALO_WIDTH = 2 # For second order derivative\n",
    "\n",
    "    tile = @cuStaticSharedMem(eltype(y_in), (TILE_DIM1, TILE_DIM2 + 2 * HALO_WIDTH))\n",
    "\n",
    "    # for tile indexing\n",
    "    k = tidx\n",
    "    l = tidy\n",
    "\n",
    "    \n",
    "\t# Writing pencil-shaped shared memory\n",
    "\n",
    "\t# for tile itself\n",
    "\tif k <= TILE_DIM1 && l <= TILE_DIM2 && i <= Ny && j <= Nx\n",
    "\t\t tile[k,l+HALO_WIDTH] = y_in[global_index]\n",
    "    end\n",
    "    \n",
    "    sync_threads()\n",
    "\n",
    "\t# for left halo\n",
    "\tif k <= TILE_DIM1 && l <= HALO_WIDTH && i <= Ny && HALO_WIDTH+1 <= j <= HALO_WIDTH + Nx \n",
    "\t\ttile[k,l] = y_in[global_index - HALO_WIDTH*Ny]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\n",
    "\t# for right halo\n",
    "\t# if k <= TILE_DIM1 && l >= TILE_DIM2 - HALO_WIDTH && HALO_WIDTH*Ny+1 <= global_index <= (Nx-HALO_WIDTH)*Ny\n",
    "\tif k <= TILE_DIM1 && TILE_DIM2 - HALO_WIDTH + 1 <= l <= TILE_DIM2 && i <= Ny && j <= Nx - HALO_WIDTH\n",
    "\t\ttile[k,l+2*HALO_WIDTH] = y_in[global_index + HALO_WIDTH*Ny]\n",
    "\tend\n",
    "\n",
    "    sync_threads()\n",
    "\n",
    "    # Finite difference operation starts here\n",
    "\n",
    "\t# Left Boundary\n",
    "\tif k <= TILE_DIM1 && l + HALO_WIDTH <= TILE_DIM2 + 2*HALO_WIDTH -2 && i <= Ny && j == 1\n",
    "\t\ty_out[global_index] = (tile[k,l + HALO_WIDTH] - 2*tile[k,l + HALO_WIDTH+1] + tile[k,l + HALO_WIDTH+2]) / h^2\n",
    "\tend\n",
    "\n",
    "\t# Center\n",
    "\tif k <= TILE_DIM1 && l + HALO_WIDTH <= TILE_DIM2 + 2*HALO_WIDTH - 1 && i <= Ny && 2 <= j <= Nx-1\n",
    "\t\ty_out[global_index] = (tile[k,l + HALO_WIDTH-1] - 2*tile[k, l + HALO_WIDTH] + tile[k,l + HALO_WIDTH + 1]) / h^2\n",
    "\tend\n",
    "\n",
    "\t# Right Boundary\n",
    "\tif k <= TILE_DIM1 && 3 <= l + HALO_WIDTH <= TILE_DIM2 + 2*HALO_WIDTH && i <= Ny && j == Nx\n",
    "\t\t@inbounds y_out[global_index] = (tile[k,l + HALO_WIDTH-2] - 2*tile[k,l + HALO_WIDTH - 1] + tile[k,l + HALO_WIDTH]) / h^2\n",
    "\tend\n",
    "\n",
    "    sync_threads()\n",
    "    \n",
    "    nothing\n",
    "end\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Performance Comparison: Through-put\n",
    "We measure the memory through-put and compare it with maximum memory bandwidth of GPU determined by its configuration. 75% efficiency is usually what we can achieve.\n",
    "\n",
    "```jl\n",
    "# tester_function_v3 : This gives evaluation on CPU, GPU_shared with error output turned off\n",
    "function tester_function_v3(f,Nx,TILE_DIM_1,TILE_DIM_2)\n",
    "    Ny = Nx\n",
    "\t@show f\n",
    "\t# @eval gpu_function = $(Symbol(f,\"_GPU\"))\n",
    "\t@eval gpu_function_shared = $(Symbol(f,\"_GPU_shared\"))\n",
    "\t# @show gpu_function\n",
    "    @show gpu_function_shared\n",
    "    h = 1/Nx\n",
    "\t# TILE_DIM_1 = 16\n",
    "\t# TILE_DIM_2 = 2\n",
    "\n",
    "\tu = randn(Nx*Ny)\n",
    "\ty_in = CuArray(u)\n",
    "\t# y_out = similar(y_in)\n",
    "\ty_out2 = similar(y_in)\n",
    "\n",
    "\tgriddim = (div(Nx,TILE_DIM_1) + 1, div(Ny,TILE_DIM_2) + 1)\n",
    "\tblockdim = (TILE_DIM_1,TILE_DIM_2)\n",
    "\n",
    "\t@show blockdim\n",
    "\t@show griddim\n",
    "\n",
    "\t# TILE_DIM = 32\n",
    "\t# THREAD_NUM = TILE_DIM\n",
    "    # BLOCK_NUM = div(Nx * Ny,TILE_DIM)+1 \n",
    "    \n",
    "\ty = f(u,Nx,Ny,h)\n",
    "\t# @cuda threads=THREAD_NUM blocks=BLOCK_NUM gpu_function(y_in, y_out, Nx, Ny, h, Val(TILE_DIM))\n",
    "    @cuda threads=blockdim blocks=griddim gpu_function_shared(y_in, y_out2, Nx, Ny, h, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "\t# @show y ≈ Array(y_out)\n",
    "\t@show y ≈ Array(y_out2)\n",
    "\n",
    "\t# @show y - Array(y_out2)\n",
    "\t\n",
    "\trep_times = 10\n",
    "\n",
    "\tt_y = time_ns()\n",
    "\tfor i in 1:rep_times\n",
    "\t\ty = f(u,Nx,Ny,h)\n",
    "\tend\n",
    "\tt_y_end = time_ns()\n",
    "\tt1 = t_y_end - t_y\n",
    "\n",
    "\tmemsize = length(u) * sizeof(eltype(u))\n",
    "\t@show Float64(t1)\n",
    "\t@printf(\"CPU Through-put %20.2f\\n\", 2 * memsize * rep_times / t1)\n",
    "\n",
    "\n",
    "\tprintln()\n",
    "\n",
    "\t# t_y_out = time_ns()\n",
    "\t# for i in 1:rep_times\n",
    "\t# \t@cuda threads=THREAD_NUM blocks=BLOCK_NUM gpu_function(y_in, y_out, Nx, Ny, h, Val(TILE_DIM))\n",
    "\t# \t# @cuda threads=THREAD_NUM blocks=BLOCK_NUM D2y_GPU_v2(y_in, y_out, Nx, Ny, h, Val(TILE_DIM))\n",
    "\t# end\n",
    "\t# synchronize()\n",
    "\t# t_y_out_end = time_ns()\n",
    "\t# t2 = t_y_out_end - t_y_out\n",
    "\t# @show Float64(t2)\n",
    "\t# @show Float64(t1)/Float64(t2)\n",
    "\t# @printf(\"GPU Through-put (naive) %20.2f\\n\", 2 * memsize * rep_times / t2)\n",
    "\n",
    "\t# println()\n",
    "\n",
    "\tt_y_out2 = time_ns()\n",
    "\tfor i in 1:rep_times\n",
    "\t\t@cuda threads=blockdim blocks=griddim gpu_function_shared(y_in, y_out2, Nx, Ny, h, Val(TILE_DIM_1), Val(TILE_DIM_2))\n",
    "\tend\n",
    "\tsynchronize()\n",
    "\tt_y_out2_end = time_ns()\n",
    "\tt3 = t_y_out2_end - t_y_out2\n",
    "\n",
    "\t@show Float64(t3)\n",
    "\t@show Float64(t1)/Float64(t3)\n",
    "\t@printf(\"GPU Through-put (shared memory)%20.2f\\n\", 2 * memsize * rep_times / t3)\n",
    "\n",
    "end\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Performance using different shapes of pencil. D2y_GPU(), Nx = Ny = 2000, tested on 1660 Ti, i5-9400F\n",
    "\n",
    "\n",
    "| TILE_DIM1 | TILE_DIM2 | Through-put (GB/s)|\n",
    "| ---------  |  --------- | --------- |\n",
    "| 16       |    16    |   116    |\n",
    "| 16       |    8    |     121   |\n",
    "| 16       |    4    |   125 |\n",
    "| 16       |    2    |    86  |\n",
    "| 16       |    1    |    47   |\n",
    "|  64      |    4    |     115    |\n",
    "|  32      |    4    |     120    |\n",
    "|  16      |    4    |     125    |\n",
    "|  8       |    4    |     81     | \n",
    "|  4       |    4    |     45     |\n",
    "| 64       |    1    |    122     |\n",
    "| 32       |    2    |    122     |\n",
    "| 16       |    4    |    125     |\n",
    "| 8        |    8    |    125     |\n",
    "| 4        |   16    |    122     |\n",
    "| 2        |   32    |   105      |\n",
    "\n",
    "\n",
    "From the table, it seems that the total size of the pencil affects the performance most, and the shape of the pencil affects the performance less.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBP_GPU package\n",
    "\n",
    "### Status\n",
    "This package is under development. It provides 2D SBP operators with accuracy orders p = 2. In the future it would contain p = 4 and p = 6\n",
    "\n",
    "### Source File\n",
    "\n",
    "```\n",
    "/src/deriv_ops.jl contains all matrix-free functions\n",
    "/src/deriv_ops_beta.jl contains all matrix-free functions with pre-allocated outputs. (Impure functions)\n",
    "/src/deriv_ops_GPU.jl contains all GPU kernels (matrix-free and impure by their nature)\n",
    "/src/deriv_ops_GPU.jl contains some functions that call corresponding kernels and return GPU arrays\n",
    "/src/deriv_ops_GPU.jl contains tester functions that output test information of both CPU and GPU operators\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Test file\n",
    "\n",
    "```\n",
    "/test/runtests.jl can be run by calling test command in package manager\n",
    "```\n",
    "\n",
    "What's in `runtests.jl`\n",
    "\n",
    "```jl\n",
    "using SBP_GPU\n",
    "using CUDA\n",
    "using Test\n",
    "using SafeTestsets\n",
    "\n",
    "@testset \"SBP_GPU.jl\" begin\n",
    "    # Write your tests here.\n",
    "    @test π ≈ 3.14 atol=0.01\n",
    "    Nx = Ny = 10\n",
    "    y_in = randn(Nx*Ny)\n",
    "    y_in_GPU = CuArray(y_in)\n",
    "    y_out_GPU = similar(y_in_GPU)\n",
    "    y_out_GPU = D2x_GPU(y_in_GPU,y_out_GPU,Nx,Ny)\n",
    "    @test Array(y_out_GPU) ≈ D2x(y_in,Nx,Ny,1/Nx)\n",
    "    @test Array(Dx_GPU(y_in_GPU,y_out_GPU,Nx,Ny)) ≈ Dx(y_in,Nx,Ny,1/Nx)\n",
    "\n",
    "    Nx = Ny = 100\n",
    "    y_in = randn(Nx*Ny)\n",
    "    y_in_GPU = CuArray(y_in)\n",
    "    y_out_GPU = similar(y_in_GPU)\n",
    "    y_out_GPU = D2x_GPU(y_in_GPU,y_out_GPU,Nx,Ny)\n",
    "    @test Array(y_out_GPU) ≈ D2x(y_in,Nx,Ny,1/Nx)\n",
    "    @test Array(Dx_GPU(y_in_GPU,y_out_GPU,Nx,Ny)) ≈ Dx(y_in,Nx,Ny,1/Nx)\n",
    "\n",
    "    Nx = Ny = 1000\n",
    "    y_in = randn(Nx*Ny)\n",
    "    y_in_GPU = CuArray(y_in)\n",
    "    y_out_GPU = similar(y_in_GPU)\n",
    "    y_out_GPU = D2x_GPU(y_in_GPU,y_out_GPU,Nx,Ny)\n",
    "    @test Array(y_out_GPU) ≈ D2x(y_in,Nx,Ny,1/Nx)\n",
    "    @test Array(Dx_GPU(y_in_GPU,y_out_GPU,Nx,Ny)) ≈ Dx(y_in,Nx,Ny,1/Nx)\n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "After running `test SBP_GPU` in package manager\n",
    "\n",
    "```jl\n",
    "(SBP_GPU) pkg> test SBP_GPU\n",
    "    Testing SBP_GPU\n",
    "Status `/tmp/jl_7hAeg6/Project.toml`\n",
    "  [052768ef] CUDA v1.3.3\n",
    "  [ba82f77b] GPUifyLoops v0.2.9\n",
    "  [da3d5682] SBP_GPU v0.1.0 `~/Github/SBP_GPU/SBP_GPU.jl`\n",
    "  ...\n",
    "\n",
    "Status `/tmp/jl_7dznRn/Manifest.toml`\n",
    "  [621f4979] AbstractFFTs v0.5.0\n",
    "  [79e6a3ab] Adapt v2.1.0\n",
    "  [b99e7846] BinaryProvider v0.5.10\n",
    "  [fa961155] CEnum v0.4.1\n",
    "  ...\n",
    "\n",
    "Test Summary: | Pass  Total\n",
    "SBP_GPU.jl    |    7      7\n",
    "    Testing SBP_GPU tests passed \n",
    "\n",
    "(SBP_GPU) pkg> \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}