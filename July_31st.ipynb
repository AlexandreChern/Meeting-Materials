{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July 31st Individual Meeting: Rework on D2y_GPU() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What slows down D2y_GPU()?\n",
    "\n",
    "```\n",
    "function D2y_GPU(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\t# tidx = ((blockIdx().x - 1) * TILE_DIM + threadIdx().x - 1)*Ny + 1\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\t# d_y = zeros(N)\n",
    "\tt1 = (tidx - 1)*Ny + 1\n",
    "\tif 1 <= t1 <= N - Ny + 1\n",
    "\t\td_y[t1] = (d_u[t1] - 2 * d_u[t1 + 1] + d_u[t1 + 2]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\tt2 = tidx * Ny\n",
    "\tif Ny <= t2 <= N\n",
    "\t\td_y[t2] = (d_u[t2 - 2] - 2 * d_u[t2 - 1] + d_u[t2]) / h^2\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\tfor j = 1:Nx\n",
    "\t\tif 2 + (j-1)*Ny <= tidx <= j*Ny-1\n",
    "\t\t\td_y[tidx] = (d_u[tidx - 1] - 2 * d_u[tidx] + d_u[tidx + 1]) / h^2\n",
    "\t\tend\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\tnothing\n",
    "end\n",
    "```\n",
    "This works, but extremely slow. Removing for loop, memory through-put for this code > 100 GB/s. We should rewrite for loop for better performance on GPU.\n",
    "\n",
    "## First rework\n",
    "\n",
    "```\n",
    "function D2y_GPU_v3(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\tif 1 <= tidx <= N && 1 <= tidx <= N\n",
    "\t\td_y[tidx] = (d_u[tidx] - 2d_u[tidx+1] + d_u[tidx + 2]) / h^2\n",
    "\telseif mod(tidx,Ny) == 0\n",
    "\t\td_y[tidx] = (d_u[tidx] - 2d_u[tidx-1] + d_u[tidx - 2]) / h^2\n",
    "\telse\n",
    "\t\td_y[tidx] = (d_u[tidx-1] - 2d_u[tidx] + d_u[tidx + 1]) / h^2\n",
    "\tend\n",
    "\tnothing\n",
    "\n",
    "end\n",
    "```\n",
    "\n",
    "This can not be executed. Error Info\n",
    "```\n",
    "julia> tester_D2y(20)\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: a exception was thrown during kernel execution.\n",
    "       Run Julia on debug level 2 for device stack traces.\n",
    "ERROR: KernelException: exception thrown during kernel execution on device GeForce GTX 1660 Ti\n",
    "Stacktrace:\n",
    " [1] check_exceptions() at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\src\\compiler\\exceptions.jl:84\n",
    " [2] prepare_cuda_call() at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\src\\state.jl:37\n",
    " [3] initialize_api() at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\cuda\\error.jl:98\n",
    " [4] macro expansion at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\cuda\\libcuda.jl:502 [inlined]\n",
    " [5] macro expansion at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\cuda\\error.jl:108 [inlined]\n",
    " [6] cuMemcpyDtoH_v2(::Ptr{Float64}, ::CuPtr{Float64}, ::Int64) at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\utils\\call.jl:93\n",
    " [7] #unsafe_copyto!#6 at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\cuda\\memory.jl:324 [inlined]\n",
    " [8] unsafe_copyto! at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\lib\\cuda\\memory.jl:317 [inlined]\n",
    " [9] unsafe_copyto! at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\src\\array.jl:309 [inlined]\n",
    " [10] copyto! at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\src\\array.jl:284 [inlined]\n",
    " [11] copyto! at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\GPUArrays\\JqOUg\\src\\host\\abstractarray.jl:102 [inlined]\n",
    " [12] collect(::CuArray{Float64,1,Nothing}) at C:\\Users\\cheny\\.juliapro\\JuliaPro_v1.4.2-1\\packages\\CUDA\\5t6R9\\src\\array.jl:264\n",
    " [13] tester_D2y(::Int64) at C:\\Users\\cheny\\OneDrive\\Documents\\version-control\\Poisson_Julia\\original_src\\deriv_ops_GPU.jl:378\n",
    " [14] top-level scope at none:0\n",
    " ```\n",
    " \n",
    " It seems that if else if is not convenient on GPU. I should try something else. </br>\n",
    " \n",
    " It's easier to do duplicate calculation than conditional statements on GPU.\n",
    " \n",
    " \n",
    " \n",
    " ## Second Rework\n",
    " \n",
    " ```\n",
    " function D2y_GPU_v4(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\tif 2 <= tidx <= N-1\n",
    "\t\td_y[tidx] = (d_u[tidx-1] - 2d_u[tidx] + d_u[tidx + 1]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\ttb = tidx * Ny\n",
    "\tif 1 <= tb <= N\n",
    "\t\td_y[tb] = (d_u[tb] - 2d_u[tb - 1] + d_u[tb - 2]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\n",
    "\tte = (tidx-1) * Ny + 1\n",
    "\tif 1 <= te <= N\n",
    "\t\td_y[te] = (d_u[te] - 2d_u[te + 1] + d_u[te + 2]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\tnothing\n",
    "end\n",
    "```\n",
    "\n",
    "This works when Ny is small (<= 111), and there are some weird bugs when Ny is larger (>= 112). Looks like some of the operations related with tb and te are not executed. I guess it should be something to do with the missing threads due to the operation tb = tidx * Ny. So I tried this and it worked without bugs:\n",
    "\n",
    "```\n",
    "function D2y_GPU_v4(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\tif 2 <= tidx <= N-1\n",
    "\t\td_y[tidx] = (d_u[tidx-1] - 2d_u[tidx] + d_u[tidx + 1]) / h^2\n",
    "\tend\n",
    "\n",
    "\n",
    "\tif 1 <= tidx <= N && mod(tidx,Ny) == 0\n",
    "\t\td_y[tidx] = (d_u[tidx] - 2d_u[tidx - 1] + d_u[tidx - 2]) / h^2\n",
    "\tend\n",
    "\n",
    "\tif 1 <= tidx <= N && mod(tidx,Ny) == 1\n",
    "\t\td_y[tidx] = (d_u[tidx] - 2d_u[tidx + 1] + d_u[tidx + 2]) / h^2\n",
    "\tend\n",
    "\tsync_threads()\n",
    "\tnothing\n",
    "end\n",
    "```\n",
    "\n",
    "## Further optimization with loop fusion.\n",
    "\n",
    "```\n",
    "function D2y_GPU_v5(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM}) where {TILE_DIM}\n",
    "\ttidx = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\tN = Nx*Ny\n",
    "\tif 2 <= tidx <= N-1\n",
    "\t\td_y[tidx] = (d_u[tidx-1] - 2d_u[tidx] + d_u[tidx + 1]) / h^2\n",
    "\tend\n",
    "\n",
    "\n",
    "\tif 1 <= tidx <= N && mod(tidx,Ny) == 0\n",
    "\t\td_y[tidx] = (d_u[tidx] - 2d_u[tidx - 1] + d_u[tidx - 2]) / h^2\n",
    "\t\td_y[tidx-Ny+1] = (d_u[tidx-Ny+1] - 2d_u[tidx - Ny + 2] + d_u[tidx - Ny + 3]) / h^2\n",
    "\tend\n",
    "\n",
    "\t# if 1 <= tidx <= N && mod(tidx,Ny) == 1\n",
    "\t# \td_y[tidx] = (d_u[tidx] - 2d_u[tidx + 1] + d_u[tidx + 2]) / h^2\n",
    "\t# end\n",
    "\tsync_threads()\n",
    "\tnothing\n",
    "end\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of D2y_GPU_v5() vs D2x_GPU()\n",
    "\n",
    "```\n",
    "julia> tester_D2y(2000)\n",
    "y ≈ y_gpu = true\n",
    "y ≈ y_gpu_2 = true\n",
    "Float64(t1) = 7.446997e8\n",
    "Float64(t2) = 5.322033e8\n",
    "Float64(t3) = 6.2246e6\n",
    "t1 / t2 = 1.3992767425530808\n",
    "t1 / t3 = 119.6381614882884\n",
    "CPU Through-put                 0.86\n",
    "GPU Through-put                 1.20\n",
    "GPU (v2) Through-put               102.82\n",
    "(7.446997e8, 5.322033e8, 6.2246e6)\n",
    "\n",
    "julia> tester_D2x(2000)\n",
    "y ≈ y_gpu = true\n",
    "y ≈ y_gpu_2 = true\n",
    "Float64(t1) = 1.0318478e9\n",
    "Float64(t2) = 7.868501e6\n",
    "Float64(t3) = 7.118e6\n",
    "t1 / t2 = 131.13651507447224\n",
    "t1 / t3 = 144.963163810059\n",
    "CPU Through-put                 0.62\n",
    "GPU Through-put                81.34\n",
    "GPU (v2) Through-put                89.91\n",
    "(1.0318478e9, 7.868501e6, 7.118e6)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Shared memory and tiling ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Interesting topics from Julia Con 2020\n",
    "\n",
    "Exascale computational group at Argonne [Exanauts](https://exanauts.github.io/)\n",
    "They did something in large-scale with Julia and GPU. [ExaPf.jl](https://github.com/exanauts/ExaPF.jl) (on Optimization Solver)\n",
    "\n",
    "Implicit Global Grid [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl)\n",
    "\n",
    "They did multiple GPU Julia code with MPI. I am particularly interested in ImplicitGlobalGrid.jl. There should be something that I learn about from their code if I want to do multiple GPU implementation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.4.2-1 1.4.2",
   "language": "julia",
   "name": "juliapro_v1.4.2-1-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
