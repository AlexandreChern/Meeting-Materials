{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August 14th Meeting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D2x_GPU_v5 (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finite Difference Operator with Shared Memory\n",
    "\n",
    "using CUDA\n",
    "\n",
    "function D2x_GPU_v5(d_u, d_y, Nx, Ny, h, ::Val{TILE_DIM1}, ::Val{TILE_DIM2}) where {TILE_DIM1, TILE_DIM2}\n",
    "\ttidx = threadIdx().x\n",
    "\ttidy = threadIdx().y\n",
    "\n",
    "\ti = (blockIdx().x - 1) * TILE_DIM1 + tidx\n",
    "\tj = (blockIdx().y - 1) * TILE_DIM2 + tidy\n",
    "\n",
    "\tglobal_index = i + (j-1)*Ny\n",
    "\n",
    "\t# i = (blockIdx().x - 1) * TILE_DIM + threadIdx().x\n",
    "\ttile = @cuStaticSharedMem(eltype(d_u),(TILE_DIM1,TILE_DIM2+4))\n",
    "\n",
    "\tk = tidx\n",
    "\tl = tidy\n",
    "\n",
    "\t# Writing pencil-shaped shared memory\n",
    "\n",
    "\t# for tile itself\n",
    "\tif k <= TILE_DIM1 && l <= TILE_DIM2 && global_index <= Nx*Ny\n",
    "\t\ttile[k,l+2] = d_u[global_index]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\t# for left halo\n",
    "\tif k <= TILE_DIM1 && l <= 2 && 2*Ny+1 <= global_index <= (Nx+2)*Ny\n",
    "\t\ttile[k,l] = d_u[global_index - 2*Ny]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\n",
    "\t# for right halo\n",
    "\tif k <= TILE_DIM1 && l >= TILE_DIM2 - 2 && 2*Ny+1 <= global_index <= (Nx-2)*Ny\n",
    "\t\ttile[k,l+4] = d_u[global_index + 2*Ny]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\t# Finite difference operation starts here\n",
    "\n",
    "\tif k <= TILE_DIM1 && l + 2 <= TILE_DIM2 + 4 && global_index <= Ny\n",
    "\t\td_y[global_index] = (tile[k,l + 2] - 2*tile[k,l+3] + tile[k,l+4]) / h^2\n",
    "\tend\n",
    "\n",
    "\tif k <= TILE_DIM1 &&  l + 2 <= TILE_DIM2 + 4 && Ny+1 <= global_index <= (Nx-1)*Ny\n",
    "\t\td_y[global_index] = (tile[k,l + 1] - 2*tile[k, l + 2] + tile[k,l+3]) / h^2\n",
    "\tend\n",
    "\n",
    "\tif k <= TILE_DIM1 && l + 2 <= TILE_DIM2 + 4 && (Nx-1)*Ny + 1 <= global_index <= Nx*Ny\n",
    "\t\td_y[global_index] = (tile[k,l] - 2*tile[k,l + 1] + tile[k,l+2]) / h^2\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "\n",
    "\tnothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but the performance is not ideal. I believe it has something to do with how I write data into shared memory.\n",
    "\n",
    "This is how it's done in C++ code. I tried something similar, but there were bugs. So I go with my current implementation that doesn't do copy past within shared memory for halo.\n",
    "\n",
    "\n",
    "```\n",
    "// fill in periodic images in shared memory array \n",
    "  if (i < 4) {\n",
    "    s_f[sj][si-4]  = s_f[sj][si+mx-5];\n",
    "    s_f[sj][si+mx] = s_f[sj][si+1];   \n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is slowing down?\n",
    "\n",
    "Set Nx = 3000\n",
    "TILE_DIM_1 = 1\n",
    "TILE_DIM_2 = 1024\n",
    "\n",
    "### The simpliest version: only reading data to shared memory\n",
    "I comment out all sections except this one\n",
    "\n",
    "```\n",
    "\t# for tile itself\n",
    "\tif k <= TILE_DIM1 && l <= TILE_DIM2 && global_index <= Nx*Ny\n",
    "\t\ttile[k,l+2] = d_u[global_index]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "```\n",
    "\n",
    "The memory through-put: 130 - 140 GB/s (Hardware maximum capacity: 188, 75%)\n",
    "\n",
    "### Add left halo\n",
    "\n",
    "I uncomment out this section\n",
    "\n",
    "```\n",
    "\t# for left halo\n",
    "\tif k <= TILE_DIM1 && l <= 2 && 2*Ny+1 <= global_index <= (Nx+2)*Ny\n",
    "\t\ttile[k,l] = d_u[global_index - 2*Ny]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "```\n",
    "\n",
    "The memory through-put 112 - 116 GB/s. Not too far from above\n",
    "\n",
    "### Add right halo\n",
    "\n",
    "```\n",
    "\t# for right halo\n",
    "\tif k <= TILE_DIM1 && l >= TILE_DIM2 - 2 && 2*Ny+1 <= global_index <= (Nx-2)*Ny\n",
    "\t\ttile[k,l+4] = d_u[global_index + 2*Ny]\n",
    "\tend\n",
    "\n",
    "\tsync_threads()\n",
    "```\n",
    "\n",
    "\n",
    "The memory though-put 107 - 108 GB/s\n",
    "\n",
    "So even our implementation of halo is not ideal here, reading data to memory isn't really something that limits the memory through-put.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Now add finite difference: Only central\n",
    "\n",
    "I un-comment this section\n",
    "\n",
    "```\n",
    "\tif k <= TILE_DIM1 &&  l + 2 <= TILE_DIM2 + 4 && Ny+1 <= global_index <= (Nx-1)*Ny\n",
    "\t\td_y[global_index] = (tile[k,l + 1] - 2*tile[k, l + 2] + tile[k,l+3]) / h^2\n",
    "\tend\n",
    "```\n",
    "\n",
    "Memory through-put dropped to 40 GB/s. By comparison, implementation without shared memory is 95 GB/s\n",
    "\n",
    "\n",
    "### Now add finite difference for left boundary data\n",
    "\n",
    "I un-comment out this section\n",
    "\n",
    "```\n",
    "\tif k <= TILE_DIM1 && l + 2 <= TILE_DIM2 + 4 && global_index <= Ny\n",
    "\t\td_y[global_index] = (tile[k,l + 2] - 2*tile[k,l+3] + tile[k,l+4]) / h^2\n",
    "\tend\n",
    "```\n",
    "\n",
    "Memory through-put is still 40 GB/s. \n",
    "\n",
    "\n",
    "### Now add finite difference for right boundary data\n",
    "\n",
    "I un-comment out this section\n",
    "\n",
    "```\n",
    "\tif k <= TILE_DIM1 && l + 2 <= TILE_DIM2 + 4 && (Nx-1)*Ny + 1 <= global_index <= Nx*Ny\n",
    "\t\td_y[global_index] = (tile[k,l] - 2*tile[k,l + 1] + tile[k,l+2]) / h^2\n",
    "\tend\n",
    "```\n",
    "\n",
    "Memory through-put drops to 30 GB/s. I don't understand why this is different from left boundary data\n",
    "\n",
    "This is the final version of D2x using shared memory. It works, but it's slower than the version without shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the shared memory version is slow?\n",
    "\n",
    "### Not re-using data from shared memory a lot\n",
    "\n",
    "We are only using second-order finite difference operator. It means each entry from shared memory is only used at most 3 times (?). For halo region, it's only used once. But the addtional cost including reading data from global memory to shared memory, reading data from shared memory for operating and write the result back to global memory. This might not be ideal. But for higher order finite difference operator, or operators that involve more entries in shared memory, (6th order , DxDy), we will see better efficiency of using shared memory\n",
    "\n",
    "\n",
    "### Data in shared memory is only used for one operator\n",
    "\n",
    "We are only using shared memory for D2x here. But the same data can be used for Dx, Or any finite difference operators in x direction. We should be able to write a kernel DX(du, dx, d2x, dxx, ...) tht read data from du once into shared memory, and output dx for Dx operator, d2x for D2x operator, ... This will also increase the utilization of shared memory.\n",
    "\n",
    "\n",
    "### Restriction from SBP operator\n",
    "We have different operators for boundary data compared to interior data in SBP operators. It might be something not very different from other schemes, but it adds complexity.\n",
    "\n",
    "For example, if finite difference operator is <br>\n",
    "(-2 1 0 0 ... , <br>\n",
    "1 -2 1 0 ... , <br>\n",
    "0 1 -2 1 0 ... , <br>\n",
    "... ) <br>\n",
    "\n",
    "We can use a unified `d_y[global_index] = (tile[k,l + 1] - 2*tile[k, l + 2] + tile[k,l+3]) / h^2` to do finite difference from shared memory. We only need to set the value of left halo from the left-most tile to be zero, and right halo from right-most tile to be zero. For tiles in the interior, we only need to do periodical memory copy to write value into halos\n",
    "\n",
    "\n",
    "But if finite difference operator is  <br>\n",
    "(1 -2 1 0 0 ... , <br>\n",
    "1 -2 1 0 0 ... , <br>\n",
    "0 1 -2 1 0 0 ... , <br>\n",
    "... ) <br>\n",
    "\n",
    "It's hard to just by alternating values in halo region to use a unified `d_y[global_index] = (tile[k,l + 1] - 2*tile[k, l + 2] + tile[k,l+3]) / h^2` to do finite difference. That's something that puzzled me for a while. In my implementation, I used three if-ends loop to write data into shared memory, and three if-end loops to read data from shared memory for finite difference. 4/6 if-end loops only involve very limited data for a very large domain and very large tile, but it looks like the cost is not proportional to the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.4.2-1 1.4.2",
   "language": "julia",
   "name": "juliapro_v1.4.2-1-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
